import hdf5plugin
import h5py
import json, random, re, pickle
from typing import List, Tuple, Dict
import numpy as np
from tqdm import tqdm
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt  # type: ignore
import matplotlib.patches as patches  # type: ignore

ATTN_H5_PATH = "pick_train_attention_last_token_keyframe.h5"
TASK_JSON    = "tasks.json"



import re, h5py
from typing import List, Tuple, Set, Iterable


def plot_tsne_from_sav(
    h5_path: str,
    episode_keys: List[str],          # è¦å¯è§†åŒ–çš„ episode åˆ—è¡¨
    labels: Dict[str, int],           # {ep_key: 1/0}
    sav_model: Dict,
    head: str | int = "best",         # "best" | "all" | å…·ä½“ head idx
    perp: int = 30,
):
    """
    head:
        "best" â†’ ç”¨ head_acc æœ€é«˜çš„ä¸€ä¸ªå¤´          (è®ºæ–‡ç¤ºä¾‹)
        "all"  â†’ concat sel_heads å…¨éƒ¨å¤´å‘é‡å† t-SNE
        int    â†’ æŒ‡å®šæŸä¸€å¤´ç´¢å¼•
    """
    sel_heads = sav_model["sel_heads"]
    head_acc  = sav_model.get("head_acc")
    if head == "best":
        head_idx = sel_heads[0] if head_acc is None else head_acc.argmax()
        head_idxs = [head_idx]
    elif head == "all":
        head_idxs = sel_heads
    elif isinstance(head, int):
        head_idxs = [head]
    else:
        raise ValueError("head å‚æ•°å¿…é¡» 'best' / 'all' / int")

    # ---- â‘  æå–ç‰¹å¾çŸ©é˜µ X ----
    X, y = [], []
    with h5py.File(h5_path, "r") as f:
        for ep in episode_keys:
            vec = episode_to_vec(load_episode(f, ep), sav_model["agg"])  # (144,256)
            feat = vec[head_idxs].reshape(-1)   # 1 å¤´=256dim; nå¤´=p*256
            X.append(feat)
            y.append(labels[ep])
    X = np.stack(X); y = np.array(y)

    # ---- â‘¡ t-SNE ----
    tsne = TSNE(n_components=2, perplexity=perp, init="pca", random_state=0)
    X_2d = tsne.fit_transform(X)

    # ---- â‘¢ ç”»å›¾ ----
    plt.figure(figsize=(6, 5))
    plt.scatter(X_2d[y == 1, 0], X_2d[y == 1, 1],
                c="red",  marker="o", label="Pick (pos)", alpha=.7, s=40)
    plt.scatter(X_2d[y == 0, 0], X_2d[y == 0, 1],
                c="blue", marker="x", label="Non-Pick (neg)", alpha=.7, s=40)

    title = (f"t-SNE â€“ head {head_idxs}" if head != 'all' else
             f"t-SNE â€“ {len(sel_heads)} heads concat")
    plt.title(title, fontsize=13)
    plt.axis("off")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"pick_tsne_{head}.png")



def split_episode_keys(
    h5_path: str,
    pos_keywords:   Set[str],       # {"\\bpick\\b"}   æ­£ç±»å…³é”®å­— (æ­£åˆ™å­—ç¬¦ä¸²)
    exclude_kw:     Set[str],       # {"\\bwipe\\b"}   ä»»ä¸€å‘½ä¸­åˆ™æ’é™¤
    train_task_set: Set[str],       # JSON è®­ç»ƒä»»åŠ¡å â†’ ç”¨äºå»é‡
) -> Tuple[List[str], List[str]]:
    """
    è¿”å› (pos_keys, neg_keys)ï¼Œé”® = "task_name/episode_xxx"
    é€»è¾‘ï¼š
      1. å…ˆæ£€æŸ¥ task æ˜¯å¦åœ¨ train_task_set â†’ è‹¥æ˜¯ç›´æ¥è·³è¿‡
      2. è‹¥ task åŒæ—¶å‘½ä¸­ pos_keywords & exclude_kw â†’ è§†ä¸ºå†²çªï¼Œè·³è¿‡
      3. è‹¥ task å‘½ä¸­ pos_keywords              â†’ æ­£ç±»
         å¦åˆ™ä¸”ä¸å« exclude_kw                 â†’ è´Ÿç±»
    """
    # --- ç¼–è¯‘å…¨éƒ¨æ­£åˆ™ ---
    pos_pats     = [re.compile(p, re.I) for p in pos_keywords]
    exclude_pats = [re.compile(p, re.I) for p in exclude_kw]

    pos_keys, neg_keys = [], []
    with h5py.File(h5_path, "r") as h5:
        for task in h5.keys():                          # é¡¶å±‚ group å
            task_norm = task.strip()

            # â‘  å»é‡ï¼šè‹¥ä¸è®­ç»ƒé›†ä»»åŠ¡åŒå â†’ è·³è¿‡
            if task_norm in train_task_set:
                continue

            # â‘¡ æ£€æŸ¥å…³é”®å­—å‘½ä¸­
            hit_pos  = any(p.search(task_norm) for p in pos_pats)
            hit_excl = any(p.search(task_norm) for p in exclude_pats)

            # â†’ å†²çªï¼šåŒæ—¶å‘½ä¸­æ­£å…³é”®è¯ & æ’é™¤å…³é”®è¯
            if hit_pos and hit_excl:
                continue       # ç›´æ¥ skip è¯¥ task

            # åˆ†ç±»
            is_pos = hit_pos

            # â‘¢ æ”¶é›† episode_xxx
            for ep in h5[task].keys():
                key = f"{task}/{ep}"
                (pos_keys if is_pos else neg_keys).append(key)

    return pos_keys, neg_keys


# ---------- è¯»å–ä»»åŠ¡åˆ†ç»„ ----------
with open(TASK_JSON, "r") as f:
    task_dict = json.load(f)
PICK_TASKS = set(task_dict["Pick_training_tasks"])
WIPE_TASKS = set(task_dict["Wipe_training_tasks"])

# ------------------------------------------------------------
# 0. æŒ‰ task åˆ’åˆ†æ­£/è´Ÿ episode key
# ------------------------------------------------------------
def collect_episode_keys(
    h5_path: str,
    pos_tasks: set,
) -> Tuple[List[str], List[str]]:
    """è¿”å› (pos_keys, neg_keys)ï¼Œé”®æ ¼å¼: task/episode_xxx"""
    pos_keys, neg_keys = [], []
    with h5py.File(h5_path, "r") as h5:
        for task in h5.keys():
            # â‘  åˆ¤æ–­è¯¥ task æ˜¯æ­£ç±»è¿˜æ˜¯è´Ÿç±»
            is_pos = task in pos_tasks
            # â‘¡ æ”¶é›†æ‰€æœ‰ episode group
            for ep in h5[task].keys():            # ep = 'episode_000', ...
                key = f"{task}/{ep}"
                (pos_keys if is_pos else neg_keys).append(key)
    return pos_keys, neg_keys

# ------------------------------------------------------------
# 1.  è¯»å–å•ä¸ª episode â†’ (F, 18, 8, 256)                â˜…
# ------------------------------------------------------------
def load_episode(h5_file, ep_key):
    grp = h5_file[ep_key]
    frames = []
    for fk in sorted(grp.keys()):
        raw = grp[fk]["last_token_attn"]
        # â€”â€” è‹¥ dtype ä¸æ˜¯ float32 å°±è½¬ â€”â€”            â˜… æ–°å¢
        arr = np.asarray(raw)
        if arr.dtype.kind == 'V' and arr.itemsize == 2:
            arr = arr.view('<f2').astype(np.float32)
        elif arr.dtype != np.float32:
            arr = arr.astype(np.float32)

        frames.append(arr)   # (18,8,256)
    return np.stack(frames, axis=0).astype(np.float32)


# ------------------------------------------------------------
# 2.  episode â†’ headsÃ—dim è¡¨ç¤º                           â˜…
# ------------------------------------------------------------
def episode_to_vec(attn_frames: np.ndarray, agg="mean") -> np.ndarray:
    """
    attn_frames: (F,18,8,256)  â†’  return (144,256)
    """
    if agg == "last":
        use_frames = attn_frames[-1:]             # (1,18,8,256)
    elif agg == "max":
        use_frames = attn_frames.max(axis=0, keepdims=True)
    else:                                         # mean
        use_frames = attn_frames.mean(axis=0, keepdims=True)
    vec = use_frames[0]                           # (18,8,256)
    L, H, D = vec.shape
    return vec.reshape(L * H, D)                  # (144,256)

# ------------------------------------------------------------
# 3.  è®¡ç®—å•å¤´å‡†ç¡®ç‡ â†’ é€‰ Top-k        (ä¸ä¹‹å‰ç›¸åŒ)
# ------------------------------------------------------------
def build_centroids(feats, labels):
    pos_cent = feats[labels == 1].mean(axis=0)
    neg_cent = feats[labels == 0].mean(axis=0)
    return pos_cent, neg_cent

def _cos(a, b):
    a_n = a / np.linalg.norm(a, axis=-1, keepdims=True)
    b_n = b / np.linalg.norm(b, axis=-1, keepdims=True)
    return (a_n * b_n).sum(-1)

def head_accuracy(feats, labels, pos_cent, neg_cent):
    sim_pos = _cos(feats, pos_cent)
    sim_neg = _cos(feats, neg_cent)
    pred     = (sim_pos > sim_neg).astype(np.int32)
    return (pred == labels[:, None]).mean(axis=0)

def select_top_heads(acc, k=20): return acc.argsort()[::-1][:k].tolist()

# ------------------------------------------------------------
# 4.  ç”¨ç¨€ç–å¤´å¤šæ•°æŠ•ç¥¨é¢„æµ‹          (ä¸ä¹‹å‰ç›¸åŒ)
# ------------------------------------------------------------
def predict_with_sparse_heads(feat, pos_c, neg_c, heads):
    def cos1(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    votes = 0
    for h in heads:
        votes += 1 if cos1(feat[h], pos_c[h]) > cos1(feat[h], neg_c[h]) else -1
    return 1 if votes > 0 else 0

# ------------------------------------------------------------
# 5.  ä¸»æµç¨‹: è®­ç»ƒ / é€‰å¤´
# ------------------------------------------------------------
def run_sav(h5_path, pos_eps, neg_eps, k=20, agg="mean"):
    with h5py.File(h5_path, "r") as f:
        feats_pos = [episode_to_vec(load_episode(f, e), agg) for e in tqdm(pos_eps)]
        feats_neg = [episode_to_vec(load_episode(f, e), agg) for e in tqdm(neg_eps)]

    feats   = np.concatenate([feats_pos, feats_neg], axis=0)           # (N,144,256)
    labels  = np.array([1]*len(feats_pos) + [0]*len(feats_neg))
    pos_c, neg_c = build_centroids(feats, labels)
    acc     = head_accuracy(feats, labels, pos_c, neg_c)               # (144,)
    heads   = select_top_heads(acc, k)

    # ---- æ–°å¢ç»Ÿè®¡ ----------------------------------------------------
    n_full = np.sum(acc == 1.0)                # 100 % å‡†ç¡®ç‡
    n_90   = np.sum(acc >= 0.9)                # â‰¥90 % å‡†ç¡®ç‡
    total_heads = acc.size                     # 144

    print(f"ğŸ¯ Top-{k} heads          : {heads}")
    print(f"âœ… Heads @100% accuracy  : {n_full}/{total_heads}")
    print(f"â­ Heads â‰¥90% accuracy   : {n_90}/{total_heads}")
    # -----------------------------------------------------------------

    return dict(
        pos_cent=pos_c,
        neg_cent=neg_c,
        sel_heads=heads,
        head_acc=acc,
        agg=agg,
        n_full=int(n_full),
    )


# ------------------------------------------------------------
# 6.  è¯„ä¼°
# ------------------------------------------------------------
def evaluate(h5_path: str, episodes: list[str], labels_dict: dict[str,int], model):
    pc, nc, heads, agg = model["pos_cent"], model["neg_cent"], model["sel_heads"], model["agg"]
    correct = 0
    with h5py.File(h5_path, "r") as f:
        for ep in tqdm(episodes):
            feat  = episode_to_vec(load_episode(f, ep), agg)
            pred  = predict_with_sparse_heads(feat, pc, nc, heads)
            correct += (pred == labels_dict[ep])
    acc = correct / len(episodes)
    print(f"âœ… Test accuracy = {acc:.3%}")

# ------------------------------------------------------------
# 4b. è¯„ä¼° Top-k æ›²çº¿ï¼ˆä¸€æ¬¡æ€§ï¼‰
# ------------------------------------------------------------
def evaluate_curve(
    h5_path: str,
    episodes: list[str],
    labels_dict: dict[str, int],
    model,
    max_k: int = 32,
    step: int = 4,
    plot_file: str | None = "topk_curve.png",
):
    """ä¸€æ¬¡æ€§è¯„ä¼° k=1..max_k çš„å‡†ç¡®ç‡å¹¶ç»˜åˆ¶æ›²çº¿ã€‚

    é€šè¿‡å‘é‡åŒ–å¤šæ•°æŠ•ç¥¨ï¼Œé¿å…é‡å¤éå†æ•°æ®é›† max_k æ¬¡ã€‚
    """

    pc, nc, head_order, agg = model["pos_cent"], model["neg_cent"], model["sel_heads"], model["agg"]

    # è‹¥ sel_heads æ²¡æŒ‰å‡†ç¡®ç‡æ’åºï¼Œåˆ™æ ¹æ® head_acc é‡æ–°æ’åº â†“
    if "head_acc" in model:
        head_order = np.argsort(model["head_acc"])[::-1]  # é™åº

    head_order = head_order[:max_k]

    # ----------1. é¢„è®¡ç®—æ¯ episode Ã— head çš„æŠ•ç¥¨ (+1/-1) ----------
    votes_all: list[np.ndarray] = []   # (N, max_k)
    with h5py.File(h5_path, "r") as f:
        for ep in tqdm(episodes, desc="precompute votes"):
            feat = episode_to_vec(load_episode(f, ep), agg)          # (144,256)
            # å–æ’åºåæ‰€éœ€å¤´å‘é‡
            sel_feat = feat[head_order]                              # (K,256)

            # ä½™å¼¦ç›¸ä¼¼åº¦æ­£è´Ÿ â†’ +1 / -1 æŠ•ç¥¨
            pos_sim = _cos(sel_feat, pc[head_order])
            neg_sim = _cos(sel_feat, nc[head_order])
            votes   = np.where(pos_sim > neg_sim, 1, -1)            # (K,)
            votes_all.append(votes)

    votes_mat = np.stack(votes_all, axis=0)      # (N, K)
    labels    = np.array([labels_dict[e] for e in episodes])  # (N,)

    # ----------2. ç´¯ç§¯å¤šæ•°æŠ•ç¥¨ & è®¡ç®—å‡†ç¡®ç‡ ----------
    cum_votes = np.cumsum(votes_mat, axis=1)  # (N,K)

    # ä»…ä¿ç•™æ¯ step ä¸ªå¤´çš„ç‚¹ï¼šä¾‹å¦‚ 4,8,...
    ks = np.arange(step, max_k + 1, step)
    idxs = ks - 1  # ç´¢å¼•ä» 0 å¼€å§‹

    preds_step = np.where(cum_votes[:, idxs] > 0, 1, 0)
    acc_k = (preds_step == labels[:, None]).mean(axis=0)  # (len(ks),)

    # ----------3. ç»˜å›¾ ----------
    if plot_file is not None:
        import matplotlib.pyplot as plt

        plt.figure(figsize=(5,3.5))
        plt.plot(ks, acc_k * 100, marker="o", linewidth=1.2, label="Accuracy")

        # å‚ç›´é»‘çº¿ï¼šè®­ç»ƒé˜¶æ®µ 100% å‡†ç¡®å¤´æ•°
        if "n_full" in model and model["n_full"] > 0:
            plt.axvline(model["n_full"], color="black", linestyle="--", label=f"Train 100% heads = {model['n_full']}")

        plt.xlabel("Top-k heads")
        plt.ylabel("Accuracy (%)")
        plt.ylim(0, 100)
        plt.grid(alpha=.3)
        plt.tight_layout()
        plt.legend()
        plt.savefig(plot_file, dpi=120)
        plt.close()

        print(f"ğŸ“ˆ Top-k æ›²çº¿å·²ä¿å­˜ â†’ {plot_file}")

    return ks, acc_k

def save_selected_head_activations(
    h5_path: str,
    episode_keys: List[str],              # è¦å¯¼å‡ºçš„ episode åˆ—è¡¨
    sel_heads: List[int],                # å·²é€‰ä¸­çš„å¤´ç´¢å¼• (ä¸€ç»´ 0~143)
    agg: str = "mean",                 # "mean" | "max" | "last"
    out_h5: str = "selected_heads_feat.h5",
    zero_fill: bool = True,              # True â†’ æœªé€‰å¤´ç”¨ 0 å¡«å……ï¼›False â†’ ä»…ä¿å­˜é€‰å¤´
):
    """æŠŠæŒ‡å®š episode çš„ Top-k å¤´æ¿€æ´»ä¿å­˜åˆ°æ–°çš„ H5 æ–‡ä»¶ä¸­ã€‚

    æ¯ä¸ª episode å°†ä¿å­˜ä¸ºä¸€ä¸ª datasetï¼Œåç§°ç”¨ "task_episode" å½¢å¼ï¼ˆæŠŠ "/" æ›¿æ¢ä¸º
    "__"ï¼‰ï¼Œshape=(144,256)ï¼ˆæˆ– (k,256) è‹¥ ``zero_fill=False``ï¼‰ã€‚
    """
    assert agg in {"mean", "max", "last"}, "agg å¿…é¡»æ˜¯ mean/max/last"

    with h5py.File(h5_path, "r") as fin, h5py.File(out_h5, "w") as fout:
        # åœ¨æ–‡ä»¶å±æ€§é‡Œè®°å½•æ‰€é€‰å¤´åŠèšåˆæ–¹å¼ï¼Œæ–¹ä¾¿åç»­è¯»å–
        fout.attrs["sel_heads"] = json.dumps(sel_heads)
        fout.attrs["agg"] = agg

        for ep in tqdm(episode_keys, desc="Saving activations"):
            feat = episode_to_vec(load_episode(fin, ep), agg)  # (144,256)

            if zero_fill:
                # è¿˜åŸä¸‰ç»´å½¢çŠ¶ (layer, head, dim) = (18,8,256)
                num_layers, num_heads = 18, 8
                data = np.zeros((num_layers, num_heads, feat.shape[-1]), dtype=np.float32)
                # å±•å¹³åæŒ‰ sel_heads å¤åˆ¶ï¼Œå† reshape å›å»
                data.reshape(-1, feat.shape[-1])[sel_heads] = feat[sel_heads]
            else:
                data = feat[sel_heads].astype(np.float32)      # (k,256)

            # ä¿æŒåŸ "task/episode" å±‚çº§ç»“æ„ï¼štask -> episode -> dataset
            task_name, ep_name = ep.split("/", 1)  # task, episode_xxx
            task_grp = fout.require_group(task_name)
            ep_grp = task_grp.require_group(ep_name)

            # å†™å…¥/è¦†ç›– dataset "selected_heads"
            if "selected_heads" in ep_grp:
                del ep_grp["selected_heads"]
            ep_grp.create_dataset("selected_heads", data=data, compression="gzip")

    print(f"âœ… å·²ä¿å­˜ {len(episode_keys)} ä¸ª episode â†’ {out_h5}")

# --------------------------- ç¤ºä¾‹è°ƒç”¨ ---------------------------
# ä¸‹é¢ç¤ºä¾‹å±•ç¤ºå¦‚ä½•æŠŠ run_sav å¾—åˆ°çš„ Top-k å¤´ (mean èšåˆ) ä¿å­˜å‡ºæ¥ã€‚
# è‹¥ä½ æƒ³å¯¼å‡º max/lastï¼Œåªéœ€è®¾ç½® agg="max"/"last" å¹¶é‡æ–°è°ƒç”¨å³å¯ã€‚
# ----------------------------------------------------------------
# example_out_h5 = "pick_topk_heads_mean.h5"
# save_selected_head_activations(
#     h5_path=ATTN_H5_PATH,
#     episode_keys=support_pos + support_neg,  # æˆ–å…¶å®ƒ episode åˆ—è¡¨
#     sel_heads=sav_model["sel_heads"],
#     agg=sav_model["agg"],
#     out_h5=example_out_h5,
#     zero_fill=True,
# )



# ------------------------------------------------------------
# 7.  è¿è¡Œç¤ºä¾‹ï¼šPick vs Non-Pick
# ------------------------------------------------------------
# 7.1  é‡‡æ ·æ”¯æŒé›†
all_pos, all_neg = collect_episode_keys(ATTN_H5_PATH, PICK_TASKS)

random.seed(42)
support_pos = random.sample(all_pos, 20)
support_neg = random.sample(all_neg, 20)

sav_model = run_sav(ATTN_H5_PATH, support_pos, support_neg, k=4, agg="mean")

pickle.dump(sav_model, open("sav_pick.pkl", "wb"))

# example_out_h5 = "steer_train_wipe_top_20_heads_mean.h5"
# save_selected_head_activations(
#     h5_path=ATTN_H5_PATH,
#     episode_keys=support_pos,  # æˆ–å…¶å®ƒ episode åˆ—è¡¨
#     sel_heads=sav_model["sel_heads"],
#     agg=sav_model["agg"],
#     out_h5=example_out_h5,
#     zero_fill=True,
# )

# 7.2  æ„é€ ï¼ˆç¤ºä¾‹ï¼‰æµ‹è¯•é›†æ ‡ç­¾
#     â€” çœŸå®å®éªŒé‡Œä½ åº”æœ‰ç‹¬ç«‹æµ‹è¯•é›†ï¼›è¿™é‡Œä»…ç¤ºèŒƒ
EVAL_H5 = "pick_eval_attention_last_token_single_action_keyframe.h5"    
all_pos_eval, all_neg_eval = split_episode_keys(EVAL_H5, pos_keywords={"\\bpick\\b"}, exclude_kw={"\\bwipe\\b"}, train_task_set=PICK_TASKS)
print(len(all_pos_eval), len(all_neg_eval)) 
# print(all_pos_eval)
# print(all_neg_eval)
pos_eval = random.sample(all_pos_eval, 200)
neg_eval = random.sample(all_neg_eval, 200)
eval_eps    = pos_eval + neg_eval          # æˆ– balanced æŠ½æ ·
eval_labels = {ep: (1 if ep in pos_eval else 0) for ep in eval_eps}
# plot_tsne_from_sav(EVAL_H5, eval_eps, eval_labels, sav_model, head="best")
evaluate(EVAL_H5, eval_eps, eval_labels, sav_model)
# ks, accs = evaluate_curve(EVAL_H5, eval_eps, eval_labels, sav_model,
#                           max_k=32, step=4, plot_file="topk_curve.png")



# ---------- Heatmap ----------


def plot_head_heatmap(acc: np.ndarray, sel_heads: List[int],
                      num_layers=18, num_heads=8, title=""):
    acc_mat = acc.reshape(num_layers, num_heads)
    fig, ax = plt.subplots(figsize=(5, 10))
    im = ax.imshow(acc_mat, cmap="RdYlBu_r", vmin=0., vmax=1.)
    ax.invert_yaxis()
    ax.set_xticks(range(num_heads)); ax.set_xticklabels([f"H{h}" for h in range(num_heads)])
    ax.set_yticks(range(num_layers)); ax.set_yticklabels([f"L{l}" for l in range(num_layers)])
    ax.set_xlabel("Head"); ax.set_ylabel("Layer"); ax.set_title(title, pad=12, weight="bold")

    # çº¢æ¡†é«˜äº®
    for idx in sel_heads:
        l, h = divmod(idx, num_heads)
        rect = patches.Rectangle((h-0.5, l-0.5), 1, 1, linewidth=1.8,
                                 edgecolor="black", facecolor="none")
        ax.add_patch(rect)

    ax.text(-0.4, -1.1,
            f"Active Heads: {len(sel_heads)}/{num_layers*num_heads}"
            f" ({len(sel_heads)/(num_layers*num_heads):.1%})",
            fontsize=8, bbox=dict(facecolor="lightgray", alpha=.7))
    cbar = plt.colorbar(im, fraction=0.046, pad=0.04)
    cbar.ax.set_ylabel("Accuracy", rotation=-90, va="bottom")
    plt.tight_layout()
    plt.savefig("SAV_head_heatmap/pick_head_heatmap_mean.png")
    plt.show()

plot_head_heatmap(sav_model["head_acc"], sav_model["sel_heads"],
                  title="Pick vs Non-Pick â€“ Head Accuracy")

